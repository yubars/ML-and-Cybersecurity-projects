{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning using CelebA Image dataset",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yubars/ML-and-Cybersecurity-projects/blob/main/Federated_Learning_using_CelebA_Image_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qpDdHCOxoaH"
      },
      "source": [
        "#Federated Learning using CelebA Image dataset\n",
        "### Deliver local models to edge devices to train and aggregate the local models to create a global model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb_QJNOQx8jk"
      },
      "source": [
        "### Learning Objectives\n",
        "* Understand the basics of Federated Learning.\n",
        "* Learn to create clients and data shards creating a federated learning environment.\n",
        "* Create a global model by aggregating weights of all the trained local models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRUq6-vkPTgR"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1gtt3xdGCvXaj40MgsdK67PGm6CFEbohF' width=500cm>\n",
        "\n",
        "Source: https://miro.medium.com/max/1400/1*HaH611vAy2eB1e42vz3X4g.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YzQ0LZtFxzI"
      },
      "source": [
        "### Imports\n",
        "Import all the required libraries for the lab including numpy, pandas, Tensorlfow, and Keras layers to create neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzpKerZ5sBLl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random\n",
        "from imutils import paths\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nCd1j-WGFk8"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "In the code cell below, we mount the google drive to the colab environment so that we have access to the local version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEgjBF3GcpM9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzs07nW3GNaz"
      },
      "source": [
        "### Read CSV\n",
        "Use Pandas to load CelebA Image CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuRpzS8Lkxpo"
      },
      "source": [
        "mydata = pd.read_csv('/content/drive/My Drive/Intro2MLDatasets/Lab8/list_attr_celeba.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKaaRL_eGlgV"
      },
      "source": [
        "### Details on Dataset\n",
        "Utilize Pandas functions to visualize the shape and first 20 instances of the dataset. More functions can be used to explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYfdX6khdNvJ"
      },
      "source": [
        "mydata.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOIILOAgdR21"
      },
      "source": [
        "mydata.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR_PR7LyG-ma"
      },
      "source": [
        "### Drop features\n",
        "Drop the features from the dataset which are not needed. Smiling is the label to be predicted and the Image ID  is used as an identifier. All the other features are excluded from the dataset because in this lab we are reading each pixel of an image to be used as a feature and the Smiling attribute as a label. The prediction depends on the model which learns from the pixels of an image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0fBoAw44UNK"
      },
      "source": [
        "mydata.drop(mydata.columns.difference(['image_id','Smiling']), 1, inplace=True)\n",
        "mydata.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdCQswlFLqw2"
      },
      "source": [
        "### Labels List\n",
        "Create a list named Label which corresponds to the feature Smiling. This list will be used later to create training and testing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7AIjjep2_hl"
      },
      "source": [
        "labels = list()\n",
        "\n",
        "# Iterate over dataframe to store target labels for attribute \"Smiling\"\n",
        "for (columnName, columnData) in mydata.iteritems():\n",
        "  if columnName == 'Smiling':\n",
        "    for i in range(0, 2988):\n",
        "      labels.append(columnData.values[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjvd-yxGMKSh"
      },
      "source": [
        "### Read image to convert into an array\n",
        "Load function reads the images from the path provided as a list to the function. It reads the image, flattens the pixel value, and scales it to value [0,1] and returns the list of lists where each list is the scaled pixel value of one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_1j0Z30obt"
      },
      "source": [
        "#Given a path, iterate over and read each image and convert to an array \n",
        "def load(path):\n",
        "  data = list()\n",
        "\n",
        "  for imgpath in path:\n",
        "    im_grayscale = cv2.imread(imgpath, 0)\n",
        "    image = np.array(im_grayscale).flatten()\n",
        "    #As the max pixel value is 255, dividing numpy array by 255 scales the image between value 0 and 1.\n",
        "    data.append(image/255)\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bHWFNiAYIi6"
      },
      "source": [
        "### Image path\n",
        "Create path for each image from the folder and call the load function. The length of image_list and labels are equal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLTRfSaoeWlF"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Intro2MLDatasets/images_celeba'\n",
        "img_paths = list(paths.list_images(image_path)) \n",
        "\n",
        "image_list = load(img_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gcwkolcYP36"
      },
      "source": [
        "### Convert labels into binary and split data\n",
        "Convert the label into binary format and split the data into train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R4rTN2znTgK"
      },
      "source": [
        "#convert the label into binary data 0 or 1\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "#Split feature and label into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, labels, test_size=0.1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcv3YpvvrZOI"
      },
      "source": [
        "### Create clients\n",
        "This function creates a dictionary with number of client devices as a key and (image pixel, label) as the value for the Federated learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPeQzmIb8_sv"
      },
      "source": [
        "\"\"\"\n",
        "Creates data shard for each client to train in local model. \n",
        "\"\"\"\n",
        "def create_clients(image_list, labels, num_clients=10):\n",
        "  client_shards_dict = {}\n",
        "\n",
        "  #list containing unique name of each client created\n",
        "  client_names = []\n",
        "  for i in range(num_clients):\n",
        "    client_names.append('{}_{}'.format('client', i+1))\n",
        "\n",
        "  #zip the image and corresponding label as a list and shuffle the data\n",
        "  data = list(zip(image_list, labels))\n",
        "  random.shuffle(data)\n",
        "  \n",
        "  #create shards of random (image, label) for each clients\n",
        "  # list of lists of tuples where (image pixel, label)\n",
        "  shards = []\n",
        "  size = len(data)//num_clients\n",
        "  for i in range(0, size*num_clients, size):\n",
        "    shards.append(data[i:i + size])\n",
        "\n",
        "  #create dictionary where key is the client name and value is the data with list of (image pixel, label) assigned to the client\n",
        "  for i in range(len(client_names)):\n",
        "    client_shards_dict[client_names[i]] = shards[i]\n",
        "\n",
        "  return client_shards_dict  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrljU_i_sJMs"
      },
      "source": [
        "### Call create_clients\n",
        "Call create_clients function to create the dictionary and save it in the variable clients. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNL3pQ7QEToi"
      },
      "source": [
        "\"\"\"\n",
        "Call create_clients function to create data shards for each client using only training datasets.\n",
        "\"\"\"\n",
        "clients = create_clients(x_train, y_train, num_clients=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qagp1Gusaln"
      },
      "source": [
        "### Create tensors\n",
        "This function takes the values from the dictionary and converts into tensor slices. It returns the shuffled tensorflow dataset in provided batch quantity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4f0Z2PZVBeW"
      },
      "source": [
        "\"\"\"\n",
        "Takes data_shard as an argument and creates a tensorflow_dataset object.\n",
        "\"\"\"\n",
        "def batch_data(data_shard):\n",
        "  data, label = zip(*data_shard)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "  return dataset.shuffle(len(label)).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv6w980gtnnD"
      },
      "source": [
        "### Call batch_data\n",
        "A dictionary is created which stores each client as the key and the tensorflow objects of batches of images and it's corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT5ZktDwU3UM"
      },
      "source": [
        "\"\"\"\n",
        "Function call batch_data to convert data shards into dataset objects which is a ready-to-use datasets for use with tensorflow framework.\n",
        "\"\"\"\n",
        "clients_batched = {}\n",
        "for (client_name, data) in clients.items():\n",
        "  clients_batched[client_name] = batch_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTCar3WzuvdE"
      },
      "source": [
        "### Create tensorflow object for test data\n",
        "As similar to the training data, we now create tensorflow object datasets for the testing purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hop6KRCFOsxG"
      },
      "source": [
        "\"\"\"\n",
        "Create tensorflow dataset object to test on the global model after several iteration of federated learning.\n",
        "\"\"\"\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jHUXtr2u7SN"
      },
      "source": [
        "### Define Multilayer Perceptron ANN\n",
        "Define a class that gives the skeleton to create the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I1kcGaOa4pZ"
      },
      "source": [
        "\"\"\"\n",
        "Define a class for Multilayer perceptron NN.\n",
        "\"\"\"\n",
        "class SimpleMLP:\n",
        "  @staticmethod\n",
        "  def build(shape, classes):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200, input_shape=(shape,), activation=\"relu\"))\n",
        "    model.add(Dense(200, activation=\"relu\"))\n",
        "    model.add(Dense(classes, activation=\"sigmoid\"))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIvkCGdrvNqS"
      },
      "source": [
        "### Initialize variables to train model\n",
        "Initialize all the variables like epoch, learning_rate, metrics to evaluate, and the optimizer to be used in the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH2smFJz07vO"
      },
      "source": [
        "#Defining total FL iterations, evaluation metric, binaryentropy loss between labels and predictions, and SGD optimizer\n",
        "FL_epochs = 2\n",
        "lr = 0.01\n",
        "loss='binary_crossentropy'\n",
        "metrics=['accuracy']\n",
        "optimizer = SGD(learning_rate = lr, decay= lr / FL_epochs, momentum = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgspOT2rvaX8"
      },
      "source": [
        "### Calculate weighted scale of dataset for each client\n",
        "For each client, calculate the proportion of dataset in the batch to the total dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1HWKFyS7MsA"
      },
      "source": [
        "def weight_scaling_factor(clients_batched, client):\n",
        "  client_names = list(clients_batched.keys())\n",
        "\n",
        "  #bs is the integer represeting number of batch\n",
        "  bs = list(clients_batched[client])[0][0].shape[0]\n",
        "  \n",
        "  #clients_cardinality comprises the cardinality of all the dataset \n",
        "  clients_cardinality = []\n",
        "  for client_name in client_names:\n",
        "    clients_cardinality.append(tf.data.experimental.cardinality(clients_batched[client_name]).numpy())\n",
        "\n",
        "  #sum of cardinality of the entire dataset\n",
        "  global_count = sum(clients_cardinality) * bs\n",
        "\n",
        "  #cardinality of a dataset specific to this client\n",
        "  local_count = tf.data.experimental.cardinality(clients_batched[client]).numpy() * bs\n",
        "\n",
        "  #return the scale of the local dataset compared to entire dataset \n",
        "  return local_count / global_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzc9J5mv0fsG"
      },
      "source": [
        "### Scale the weights\n",
        "The function below returns the list of weight for the local model after it is scaled to the scaling factor generated from the above function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQObNMdm7Mes"
      },
      "source": [
        "#Returns the list of lists of weights scaled to the specific client dataset scale \n",
        "def scale_model_weights(weight, scalar):\n",
        "  final_weight = []\n",
        "  steps = len(weight)\n",
        "  for i in range(steps):\n",
        "    final_weight.append(weight[i] * scalar)  #weight[i] is a list of weights\n",
        "  return final_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzRbTfPU1Pmc"
      },
      "source": [
        "### Calculate average weights from the local  models\n",
        "Given the list of list containing weights for each local model, an average model weight is calculated to update the global model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O06WYaxW7MGz"
      },
      "source": [
        "#Returns the list of average_weights for the global model \n",
        "def sum_scaled_weights(scaled_local_weight_list):\n",
        "  average_weights = list()\n",
        "  for weight_list_tuple in zip(*scaled_local_weight_list):\n",
        "    layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0) #adds corresponding weight of every model which is mean\n",
        "    average_weights.append(layer_mean.numpy())\n",
        "  \n",
        "  return average_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRfGreBT1ha8"
      },
      "source": [
        "### Evaluate the model\n",
        "The function below helps evaluate the model with accuracy and loss metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARgVKwTb_-un"
      },
      "source": [
        "#Test model to make predictions on the available test dataset \n",
        "def test_model(x_test, y_test, model, epoch):\n",
        "  cce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "  pred = model.predict(x_test)\n",
        "  for ind,item in enumerate(pred):\n",
        "    if item[0] > 0.5:\n",
        "      item[0] = 1\n",
        "    else:\n",
        "      item[0] = 0\n",
        "\n",
        "  loss = cce(y_test, pred)\n",
        "  acc = accuracy_score(y_test, pred)\n",
        "  print('epoch: {} | global_acc: {:.2%} | global_loss: {}'.format(epoch, acc, loss))\n",
        "  return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwT56Mug1xFu"
      },
      "source": [
        "### Create global model\n",
        "The code below is the main of the program which creates the global model. For each client a local model is created and trained with the corresponding batch data. Each model weights of the clients are then calculated, scaled, and aggregated to update the weight of the global model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SprvTI383A2T"
      },
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "global_MLP = SimpleMLP()\n",
        "#CelebA image shape is (218, 178, 3), Grayscale CelebA image shape is (218, 178, 1)\n",
        "global_model = global_MLP.build(38804, 1)\n",
        "\n",
        "for FL_epoch in range(FL_epochs):\n",
        "  global_weights = global_model.get_weights()\n",
        "  scaled_local_weight_list = []\n",
        " \n",
        "  #randomly shuffle the client names\n",
        "  client_names = list(clients_batched.keys())\n",
        "  random.shuffle(client_names)\n",
        "\n",
        "  \"\"\"\n",
        "  For each client, creates a local model and sets the weight to the global model weights.\n",
        "  Trains the model using the data shard for the specific client.\n",
        "  Generates the new local weight from the model and scales it to the fraction of the dataset.\n",
        "  \"\"\"\n",
        "  \n",
        "  for client in client_names:\n",
        "    local_MLP = SimpleMLP()\n",
        "    local_model = local_MLP.build(38804, 1)\n",
        "    local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    local_model.set_weights(global_weights)\n",
        "    local_model.fit(clients_batched[client], epochs=1)\n",
        "   \n",
        "    scaling_factor = weight_scaling_factor(clients_batched, client)\n",
        "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "    scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "    K.clear_session()\n",
        "  \n",
        "  #The list of lists containing weights of each local model is then reverse zipped and aggregated weight of each layer from all the local model is calculated\n",
        "  average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    \n",
        "  global_model.set_weights(average_weights)\n",
        "\n",
        "  for(xt, yt) in test_batched:\n",
        "    global_acc, global_loss = test_model(xt, yt, global_model, FL_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uIaexFGCN9B"
      },
      "source": [
        "**What is the advantage of using Federated Learning? [3 points]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhGK2SaUclOY"
      },
      "source": [
        "Some the benifits provided by Federated leaning are: \n",
        "\n",
        "1.   Maintains users' privacy\n",
        "2.   Redcues bandwidth as it doesnot share data but only parameters\n",
        "1.   Enhances security\n",
        "2.   Provides colaborative learning \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBeIspoC2VJ"
      },
      "source": [
        "**How can we achieve security and privacy using Federated Learning in ML based systems? [5 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxrQSH0yfOWf"
      },
      "source": [
        "Federated Learning is machine learning techniques where there are local models in clients and gloabal model in server/agent. Clients or distributed nodes collect user data, train models and send parametrs only rather than user's data to agent/server. On the basis of parameter from federated nodes, global model update and send updated weight to distributed nodes.\n",
        "\n",
        "So, in federated learning, distributed nodes send only parameters rather than users' data to server. Data remains in users' nodes maintaining privacy. Since, local model is not sending user's sensative data, its difficult for intruder/hacker to get data neither from transmission channel nor from server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlr-GChvDIsK"
      },
      "source": [
        "**What are the possible ways that one can use for updating global model parameters in Federated Learning? [2 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdLsIWqzi2ny"
      },
      "source": [
        "Once local parameters from federated nodes receive in server, global model update model parameters by aggregating weights from trained local model. The most common way is to update model with weightage avearage as in Figure below. The next step is calculate performances of model and accepted if better than previous one. finally, updated final global parameers are send to local nodes/model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNWNcO_cFuT"
      },
      "source": [
        "\n",
        "**What are the challenges you see in FL? (1 point) ]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh6EZd5pjhHZ"
      },
      "source": [
        "The challenges in Federated learning are: \n",
        "\n",
        "*   Training in loacal nodes as some of them might be reseorce constraints\n",
        "*   Poisoning global model parameters by malicious local nodes\n"
      ]
    }
  ]
}