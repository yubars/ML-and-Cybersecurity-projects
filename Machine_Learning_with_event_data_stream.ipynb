{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning with event data stream ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yubars/ML-and-Cybersecurity-projects/blob/main/Machine_Learning_with_event_data_stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFCn-xQwg0Gh"
      },
      "source": [
        "# Online Learning using Apache Kafka\n",
        "#### Using Apache Kafka to simulate real time data stream model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18H_dm9etzO6"
      },
      "source": [
        "### Learning Objecives\n",
        "* Learn to import kafka and write to and read from kafka.\n",
        "* Learn to use layers in keras.\n",
        "* Learn to use kafka for data streams and train the model in real-time.\n",
        "* Learn the concept of online learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZhPKWnzhR_w"
      },
      "source": [
        "### Install Package\n",
        "Install Kafka and Tensorflow IO. Tensorflow IO is an extension package to Tensorflow which supports integration with Apache Kafka and other systems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39jwbs8spzhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1533534-a0cb-45dd-d59c-f27fe2f64591"
      },
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: tensorflow<2.7.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.21.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.37.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.41.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.6.0)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.7/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14eihOmhdlZ"
      },
      "source": [
        "### Imports\n",
        "Import all the required libraries for the lab including Kafka, pandas, Tensorflow, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFgauMIEsb8b"
      },
      "source": [
        "import time \n",
        "from kafka import KafkaProducer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ybkg2_UiwLQ"
      },
      "source": [
        "### Download Kafka\n",
        "Download and setup Kafka for real time data stream simulation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB7oqVR7xlsm"
      },
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
        "!tar -xzf kafka_2.12-2.8.1.tgz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV6xtkye4WBV"
      },
      "source": [
        "### Start running Kafka and Zookeeper server instances \n",
        "\n",
        "Start Kafka and Zookeeper servers as a daemon processes. Zookeeper is a centralized service for maintaing configuration information, naming, providing distributed synchronization, and providing group services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMc545ia_C9N"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-2.8.1/config/zookeeper.properties\n",
        "!./kafka_2.12-2.8.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-2.8.1/config/server.properties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_VBN2I7f0V"
      },
      "source": [
        "### Create a topic to store events\n",
        "Create topic for train and test dataset to store events in Kafka. Kafka is a distributed event streaming platform that lets you read, write, store and process events. These events or messages are organized and stored in topics. In simple terms, topic is similar to a folder in a filesystem, and the events are the file in that folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8EBXCVYAG70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f232d77d-6aed-4ee5-ebf6-bb011fc401c9"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-train --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-test --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while executing topic command : Topic 'susy-train' already exists.\n",
            "[2021-10-29 14:40:03,807] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'susy-train' already exists.\n",
            " (kafka.admin.TopicCommand$)\n",
            "Error while executing topic command : Topic 'susy-test' already exists.\n",
            "[2021-10-29 14:40:06,842] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'susy-test' already exists.\n",
            " (kafka.admin.TopicCommand$)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx5BZYklEDiI"
      },
      "source": [
        "### Describe the topic for details\n",
        "Describe command helps us gather details on topic, it's partitions, replicas, and other important information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXLs0nkvDaUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250443ae-4986-4044-919c-ac4be659b306"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-train --bootstrap-server localhost:9092\n",
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-test --bootstrap-server localhost:9092"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: susy-train\tTopicId: gJ-7Te6ESm2yQRMnIUTfoA\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-train\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "Topic: susy-test\tTopicId: iuLcF4x4R52j-d14TfVf6g\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-test\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "\tTopic: susy-test\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-NJOBi6Ea-6"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "In the code cell below, we mount the google drive to the colab environment so that we have access to the local version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfYkP4mEBM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56529775-15c9-48e0-a245-263efca168fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36sU7_q2EgTj"
      },
      "source": [
        "### Define features\n",
        "COLUMNS is used to define each of the feature in the SUSY dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mMIPL9cKh_1"
      },
      "source": [
        "COLUMNS = [\n",
        "           'class',\n",
        "           'lepton_1_pT',\n",
        "           'lepton_1_eta',\n",
        "           'lepton_1_phi',\n",
        "           'lepton_2_pT',\n",
        "           'lepton_2_eta',\n",
        "           'lepton_2_phi',\n",
        "           'missing_energy_magnitude',\n",
        "           'missing_energy_phi',\n",
        "           'MET_rel',\n",
        "           'axial_MET',\n",
        "           'M_R',\n",
        "           'M_TR_2',\n",
        "           'R',\n",
        "           'MT2',\n",
        "           'S_R',\n",
        "           'M_Delta_R',\n",
        "           'dPhi_r_b',\n",
        "           'cos(theta_r1)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbBS3M_FYx8"
      },
      "source": [
        "### SUSY Dataset\n",
        "\n",
        "SUSY Data set is produced using Monte Carlo simulations. It is the data produced from the particle accelerators. The first column of the dataset is the label followed by 8 features which are kinematic properties measured by the particle detectors in the accelerator. The last 10 features are the high-level features derived by physicists to help discriminate between the two classes signal process or a background process.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjgFSdAEyuK"
      },
      "source": [
        "### Read CSV\n",
        "Use Pandas to load SUSY dataset from the CSV file and provide the column name for each of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ6F6wlsJCMJ"
      },
      "source": [
        "mydata = pd.read_csv('/content/drive/My Drive/Intro2MLDatasets/Lab6/SUSY.csv', header=None, names=COLUMNS, nrows=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7PBfl_AFJBH"
      },
      "source": [
        "### Visualize Data\n",
        "\n",
        "Panda functions helps us visualize the SUSY dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqi_w6yhKNT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "b288b8f1-ee0a-40c8-dac8-43d981973ba6"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>lepton_1_pT</th>\n",
              "      <th>lepton_1_eta</th>\n",
              "      <th>lepton_1_phi</th>\n",
              "      <th>lepton_2_pT</th>\n",
              "      <th>lepton_2_eta</th>\n",
              "      <th>lepton_2_phi</th>\n",
              "      <th>missing_energy_magnitude</th>\n",
              "      <th>missing_energy_phi</th>\n",
              "      <th>MET_rel</th>\n",
              "      <th>axial_MET</th>\n",
              "      <th>M_R</th>\n",
              "      <th>M_TR_2</th>\n",
              "      <th>R</th>\n",
              "      <th>MT2</th>\n",
              "      <th>S_R</th>\n",
              "      <th>M_Delta_R</th>\n",
              "      <th>dPhi_r_b</th>\n",
              "      <th>cos(theta_r1)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972861</td>\n",
              "      <td>0.653855</td>\n",
              "      <td>1.176225</td>\n",
              "      <td>1.157156</td>\n",
              "      <td>-1.739873</td>\n",
              "      <td>-0.874309</td>\n",
              "      <td>0.567765</td>\n",
              "      <td>-0.175000</td>\n",
              "      <td>0.810061</td>\n",
              "      <td>-0.252552</td>\n",
              "      <td>1.921887</td>\n",
              "      <td>0.889637</td>\n",
              "      <td>0.410772</td>\n",
              "      <td>1.145621</td>\n",
              "      <td>1.932632</td>\n",
              "      <td>0.994464</td>\n",
              "      <td>1.367815</td>\n",
              "      <td>0.040714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.667973</td>\n",
              "      <td>0.064191</td>\n",
              "      <td>-1.225171</td>\n",
              "      <td>0.506102</td>\n",
              "      <td>-0.338939</td>\n",
              "      <td>1.672543</td>\n",
              "      <td>3.475464</td>\n",
              "      <td>-1.219136</td>\n",
              "      <td>0.012955</td>\n",
              "      <td>3.775174</td>\n",
              "      <td>1.045977</td>\n",
              "      <td>0.568051</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448410</td>\n",
              "      <td>0.205356</td>\n",
              "      <td>1.321893</td>\n",
              "      <td>0.377584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444840</td>\n",
              "      <td>-0.134298</td>\n",
              "      <td>-0.709972</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>-1.613871</td>\n",
              "      <td>-0.768661</td>\n",
              "      <td>1.219918</td>\n",
              "      <td>0.504026</td>\n",
              "      <td>1.831248</td>\n",
              "      <td>-0.431385</td>\n",
              "      <td>0.526283</td>\n",
              "      <td>0.941514</td>\n",
              "      <td>1.587535</td>\n",
              "      <td>2.024308</td>\n",
              "      <td>0.603498</td>\n",
              "      <td>1.562374</td>\n",
              "      <td>1.135454</td>\n",
              "      <td>0.180910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.381256</td>\n",
              "      <td>-0.976145</td>\n",
              "      <td>0.693152</td>\n",
              "      <td>0.448959</td>\n",
              "      <td>0.891753</td>\n",
              "      <td>-0.677328</td>\n",
              "      <td>2.033060</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>3.046260</td>\n",
              "      <td>-1.005285</td>\n",
              "      <td>0.569386</td>\n",
              "      <td>1.015211</td>\n",
              "      <td>1.582217</td>\n",
              "      <td>1.551914</td>\n",
              "      <td>0.761215</td>\n",
              "      <td>1.715464</td>\n",
              "      <td>1.492257</td>\n",
              "      <td>0.090719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.309996</td>\n",
              "      <td>-0.690089</td>\n",
              "      <td>-0.676259</td>\n",
              "      <td>1.589283</td>\n",
              "      <td>-0.693326</td>\n",
              "      <td>0.622907</td>\n",
              "      <td>1.087562</td>\n",
              "      <td>-0.381742</td>\n",
              "      <td>0.589204</td>\n",
              "      <td>1.365479</td>\n",
              "      <td>1.179295</td>\n",
              "      <td>0.968218</td>\n",
              "      <td>0.728563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.083158</td>\n",
              "      <td>0.043429</td>\n",
              "      <td>1.154854</td>\n",
              "      <td>0.094859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  lepton_1_pT  lepton_1_eta  ...  M_Delta_R  dPhi_r_b  cos(theta_r1)\n",
              "0    0.0     0.972861      0.653855  ...   0.994464  1.367815       0.040714\n",
              "1    1.0     1.667973      0.064191  ...   0.205356  1.321893       0.377584\n",
              "2    1.0     0.444840     -0.134298  ...   1.562374  1.135454       0.180910\n",
              "3    1.0     0.381256     -0.976145  ...   1.715464  1.492257       0.090719\n",
              "4    1.0     1.309996     -0.690089  ...   0.043429  1.154854       0.094859\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrX5owSuGmFo"
      },
      "source": [
        "### Split Train and Test data\n",
        "As always, it is necessary to split the data into train, test, and validation. In this context, we are splitting 80% of the data to be the train data. The remaining 20% of the data is split between test and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lizEmDGHMO4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c44f07-9740-4258-c073-d202affc3fb1"
      },
      "source": [
        "train_df, test_df = train_test_split(mydata, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"class\"], axis=1)\n",
        "y_train_df = train_df[\"class\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"class\"], axis=1)\n",
        "y_test_df = test_df[\"class\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  70000\n",
            "Number of testing sample:  30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_7J7ViHJJw"
      },
      "source": [
        "### Convert data to list format\n",
        "Read each row from the dataframe and convert it to the list format to feed to Kafka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36wbZHBmTWcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d68e71-ca42-4f58-f9c4-18b0c80d05b4"
      },
      "source": [
        "#Convert each test and train dataframe to list form to feed to kafka\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 70000, 30000, 30000)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR9ptIMmaLNH"
      },
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdX-5m7CJZZl"
      },
      "source": [
        "### Create Kafka Producer \n",
        "Create Kafka producer which takes in data and sends the record to the partition within a topic in Kafka cluster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVGpokJzWDHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14e0417-d5c7-4ec7-f485-f689b9ab3283"
      },
      "source": [
        "#send each record to a partition within a topic in kafka cluster\n",
        "def write_to_kafka(topic, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic, key=key.encode('utf-8'), value=message.encode('utf-8'))\n",
        "    count += 1 \n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic))\n",
        "\n",
        "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"susy-test\", zip(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 70000 messages into topic: susy-train\n",
            "Wrote 30000 messages into topic: susy-test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJcUt38yKwrn"
      },
      "source": [
        "### Online Learning\n",
        "Unlike traditional training of machine learning models, online learning is based on incrementally learning or updating parameters as soon as the new data points are available. This process continues indefinitely. In the code below, stream_timeout is set to 10000 milliseconds which means as all the messages are consumed from the topic, the dataset will wait for 10 more seconds before timing out and disconnecting from the Kafka cluster. If additional data arrives in that time period, model training resumes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tritmkkgwv9x"
      },
      "source": [
        "online_train_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-train\"],\n",
        "    group_id=\"cgonline\",\n",
        "    servers=\"localhost:9092\",\n",
        "    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_online_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "online_train_ds = online_train_ds.shuffle(buffer_size=32)\n",
        "online_train_ds = online_train_ds.map(decode_kafka_online_item)\n",
        "online_train_ds = online_train_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5VFMGf9J6QG"
      },
      "source": [
        "### Initialize variables to create ANN\n",
        "Initialize Optimizer, loss function, metrics function, and epochs for Neural Network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOSkIPwgEOYE"
      },
      "source": [
        "OPTIMIZER=\"adam\"\n",
        "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "METRICS = ['accuracy']\n",
        "EPOCHS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5f_oGoOKXEI"
      },
      "source": [
        "### Define Model\n",
        "Create Neural network containing multiple layers and use Dropout layer to prevent overfitting in model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8OZdXdXJ26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8b4722-8cae-487d-aca4-1efcea1aa059"
      },
      "source": [
        "#define model input shape, and layers of NN\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(128, input_shape=(NUM_COLUMNS,), activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 128)               2432      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 68,481\n",
            "Trainable params: 68,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIOEbm5Zxx1"
      },
      "source": [
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9NmCVdIeSG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ea51c0-ecde-4339-9a35-c0d3af70fee5"
      },
      "source": [
        "model.fit(online_train_ds, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2188/2188 [==============================] - 32s 15ms/step - loss: 0.4747 - accuracy: 0.7767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79bf009510>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUxQs72-MMGn"
      },
      "source": [
        "### Prepare test data\n",
        "\n",
        "Prepare the test dataset using KafkaGroupIODataset to stream and test with the model we initialized before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMLaZdqte2ak"
      },
      "source": [
        "#prepare test data to evaluate on the online trained model\n",
        "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-test\"],\n",
        "    group_id=\"testcg\",\n",
        "    servers=\"localhost:9092\",\n",
        "    stream_timeout=10000,\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_test_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "test_ds = test_ds.map(decode_kafka_test_item)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EDZ9PN8SARD"
      },
      "source": [
        "### Evaluate Model\n",
        "Use predefined function evaluate to figure loss value and metric value with the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj2rb9p3ylS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65b74f4-5316-4700-ca05-0ef30ba3dfdc"
      },
      "source": [
        "res = model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 23s 25ms/step - loss: 0.4498 - accuracy: 0.7917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xODSoAr9bnFf"
      },
      "source": [
        "loss: 0.4488 - accuracy: 0.7918\n",
        "loss: 0.4476 - accuracy: 0.7905"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlar0_-783K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2121cda-81b2-421a-c810-5755f149ceaa"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group cgonline\n",
        "!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group testcg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consumer group 'cgonline' has no active members.\n",
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
            "cgonline        susy-train      0          280000          280000          0               -               -               -\n",
            "\n",
            "Consumer group 'testcg' has no active members.\n",
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
            "testcg          susy-test       0          64768           64768           0               -               -               -\n",
            "testcg          susy-test       1          55232           55232           0               -               -               -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slTfvrP0lPix"
      },
      "source": [
        "**What is the difference between this Lab (Lab 6) and Lab 4 (IDS)? [5 points]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2LDKVGqk3_"
      },
      "source": [
        "There are following differences, I observed,  between Lab 4 and this lab (Lab 6)\n",
        "\n",
        "1.   In this lab, we train the model in real-time from live data streams but in Lab 4, we trained from historical dataset\n",
        "2.   In this lab, We use a number of layers from keras (deep neural network) to build a model whereas we had used the Random Forest classifier in Lab 4\n",
        "3.   In this lab, we installed an Apache Kafka server  and trained a model for online learning which is different from the general model that we developed in Lab4 in the sense that it incrementally trains model and update parameters when new data points (from stream) are available.\n",
        "4.   We created columns heading for dataset in this lab and imported SUSY dataset in this lab whereas in Lab4, we imported KDD intrusion dataset with column heading from drive\n",
        "5.   We had worked on an imbalanced dataset in Lab4 which was later made balanced with SMOTE methods but here we have not done so.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkRNwArNErT0"
      },
      "source": [
        "**Did you observe any differences in result during the evlaution of the model when you rerun? [5 points]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1-rI5BdBwi"
      },
      "source": [
        "####When I rerun the evaluate section only, it pops up an overflow error, meaning there is no more data in kafka server to read.\n",
        "####But when I run all programs, it runs without significant difference in both evaluation metrics, loss and accuracy. The test loss/accuracy is almost the same as training loss/accuracy, so the model is performing well. While reruning with runall (entire programs), kafka server takes a new data stream, trains the model with those new data points, updates the parameters and evaluates for new test data so that offset value increases from previous. "
      ]
    }
  ]
}